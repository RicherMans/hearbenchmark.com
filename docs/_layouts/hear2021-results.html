---
layout: default
---
<article>
    <div class="Margins">
        <div class="center">
            <div style="padding-top: 50px; padding-bottom: 50px;">
                <img src="/assets/img/hear-header-sponsor.jpg">
            </div>
            <h1 class="LatexTitle">
                {{ page.title }}
                {%- if page.subtitle -%}
                    <br/> {{ page.subtitle }}
                {%- endif -%}
            </h1>
            <!-- <p class="LatexName">{{ page.author }}</p> -->
            <p class="LatexDate">
                {{ page.date | date: '%B %-d, %Y' }}
            </p>
            {%- if page.abstract -%}
            <div class="AbstractMargins">
                <h3 class="LatexAbstract">
                    Abstract
                </h3>
                <p class="LatexAbstractText">{{page.abstract}}</p>
            </div>
            {%- endif -%}

        </div>

        <div class="divider"></div>

        <p>
            These are the final evaluation scores for HEAR 2021. All model submissions
            have been evaluated on a set of open and secret task from a variety of
            different audio domains including speech, music, and environmental sounds.
        </p>

        <p>Downstream evaluation on each task involves two
        steps: a) computing audio
        embeddings and b) learning a shallow fully-connected
        predictor. The downstream predictor was chosen to optimize
        validation scores on the task's objective, measured on the
        validation set.</p>

        <p>
        For more details, question, training logs, etc, don't
        hesitate to <a
        href="https://discuss.neuralaudio.ai/c/hear-2021-neurips-challenge">post</a>
        on the discussion board or <a href="mailto:deep at neuralaudio
        dot ai">email us</a>.</p>

        <hr class="divider-line"/>

        <!-- TOC for tasks is populated using JS based on tasks below -->
        <div>
            <p>
                <b>Open Tasks:</b>
            </p>
            <div id="open-task-toc"></div>
            <p>
                <b>Secret Tasks:</b>
            </p>
            <div id="secret-task-toc"></div>
        </div>

        <hr class="divider-line"/>

        <div id="tasks-wrapper">

            <!-- DCASE -->
            <div id="dcase2016_task2-hear2021-full" class="leaderboard-task">
                <h2 class="task-title open-task">DCASE 2016 Task 2</h2>
                <div class="task-description">
                    <p>
                        Adapted from <a
                        href="http://dcase.community/challenge2016/task-sound-event-detection-in-synthetic-audio">DCASE
                        2016, Task 2</a> office sound event detection. Our
                        evaluation uses different splits,
                        so the numbers cannot be directly compared
                        to previously published results.</p>
                        <p>Postprocessing: Segments were postprocessed
                        using 250 ms median filtering. At each
                        validation step, a minimum event duration
                        of 125 or 250 ms was chosen to maximize
                        onset-only event-based F-measure (with 200ms
                        tolerance). Scores were computed using <a
                                    href="https://tut-arg.github.io/sed_eval/">sed_eval</a>.
                    </p>
                </div>
                {% include html_result_tables/dcase2016_task2-hear2021-full.html %}
            </div>

            <!-- NSynth Pitch 5h -->
            <div id="nsynth_pitch" class="leaderboard-task">
                <h2 class="task-title open-task">NSynth Pitch</h2>
                <div class="task-description">
                    <p>Pitch and chroma classification of <a
                    href="https://magenta.tensorflow.org/datasets/nsynth">nsynth
                    sounds</a>.</p>
                </div>
                <div id="nsynth_pitch-v2.2.3-5h" class="subtask">
                    <h3>5 Hour Dataset</h3>
                    {% include html_result_tables/nsynth_pitch-v2.2.3-5h.html %}
                </div>
                <div id="nsynth_pitch-v2.2.3-50h" class="subtask">
                    <h3>50 Hour Dataset</h3>
                    {% include html_result_tables/nsynth_pitch-v2.2.3-50h.html %}
                </div>
            </div>

            <!-- Speech Commands 5hr -->
            <div id="speech_commands" class="leaderboard-task">
                <h2 class="task-title open-task">Speech Commands</h2>
                <div class="task-description">
                    <p><a href="https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html">Speech
                    commands classification</a>. In this task only 5 hours of audio was used
                    for training and validation.
                    </p>
                </div>
                <div id="speech_commands-v0.0.2-5h" class="subtask">
                    <h3>5 Hour Dataset</h3>
                    {% include html_result_tables/speech_commands-v0.0.2-5h.html %}
                </div>
                <div id="speech_commands-v0.0.2-full" class="subtask">
                    <h3>Full Dataset</h3>
                    {% include html_result_tables/speech_commands-v0.0.2-full.html %}
                </div>

            </div>

            <!-- Beehive states -->
            <div id="beehive_states" class="leaderboard-task">
                <h2 class="task-title secret-task">Beehive States</h2>
                <div class="task-description">
                    <p>This is a binary classification task using audio recordings
                    of two beehives. The beehives are in one of two states: a Queen-less
                    beehive, where for some reason the Queen is missing, and a normal
                    beehive.</p>
                    <p>This task has been split into two folds, where all the samples
                    from each hive are in only one of the folds. Training and testing
                    was performed on each.</p>
                    <p>Additionally, this task has been split into two different sizes:
                    a 5hr version and a full dataset version.</p>
                    <p>Adopted from: <a href="https://zenodo.org/record/2667806#.YafvMPHMJpQ">
                        Audio-Based identification of Beehive states: The dataset
                    </a></p>
                </div>

                <div id="beehive_states_fold0-v2-5h" class="subtask">
                    <h3>HEAR 2021 - 5 Hour Dataset - Fold 0</h3>
                    {% include html_result_tables/beehive_states_fold0-v2-5h.html %}
                </div>
                <div id="beehive_states_fold1-v2-5h" class="subtask">
                    <h3>HEAR 2021 - 5 Hour Dataset - Fold 1</h3>
                    {% include html_result_tables/beehive_states_fold1-v2-5h.html %}
                </div>
                <div id="beehive_states_fold0-v2-full" class="subtask">
                    <h3>Full Dataset - Fold 0</h3>
                    {% include html_result_tables/beehive_states_fold0-v2-full.html %}
                </div>
                <div id="beehive_states_fold1-v2-full" class="subtask">
                    <h3>Full Dataset - Fold 1</h3>
                    {% include html_result_tables/beehive_states_fold1-v2-full.html %}
                </div>
            </div>

            <!-- Beijing Opera -->
            <div id="beijing_opera-v1.0-hear2021-full" class="leaderboard-task">
                <h2 class="task-title secret-task">Beijing Opera Percussion - HEAR 2021</h2>
                <div class="task-description">
                    <p>This is a novel audio classification task developed using the
                    <a href="https://zenodo.org/record/1285212#.YafyDvHMJpQ">Beijing
                        Opera Percussion Instrument Dataset</a>. The Beijing Opera
                    uses six main percussion instruments that can be classified into
                    four main categories: Bangu, Naobo, Daluo, and Xiaoluo.</p>
                    <p>Scores are averaged over 5-folds.</p>
                </div>
                {% include html_result_tables/beijing_opera-v1.0-hear2021-full.html %}
            </div>

            <!-- CREMA-D -->
            <div id="tfds_crema_d-1.0.0-full.html" class="leaderboard-task">
                <h2 class="task-title secret-task">CREMA-D</h2>
                <div class="task-description">
                    <p><a href="https://github.com/CheyneyComputerScience/CREMA-D">CREMA-D</a>
                        is a dataset for emotion recognition. The original dataset
                    contains audiovisual data of actors reciting sentences with one of
                    six different emotions (Anger, Disgust, Fear, Happy, Neutral and Sad).
                    For HEAR 2021 we only use the audio recordings.</p>
                    </p>
                    <p>We use the <a href="https://www.tensorflow.org/datasets/catalog/crema_d">
                    TensorFlow Dataset</a> version of this dataset, however compute our
                        scores over our own 5-fold split.
                    </p>
                </div>
                {% include html_result_tables/tfds_crema_d-1.0.0-full.html %}
            </div>

            <!-- ESC50 5hr -->
            <div id="esc50" class="leaderboard-task">
                <h2 class="task-title secret-task">ESC-50</h2>
                <div class="task-description">
                    <p>This is a multiclass classification task on environmental sounds.
                        The <a href="https://github.com/karoldvl/ESC-50">ESC-50</a> dataset is a
                        collection of 2000 environmental sounds organized into 50
                        classes.</p>
                    <p>Results are averaged over five folds, which are predefined in the
                    original dataset.</p>
                    <p>In additional to the full dataset, we also include a smaller 5
                    hour version for HEAR 2021.</p>
                </div>

                <div id="esc50-v2.0.0-5h" class="subtask">
                    <h3>5 Hour Dataset - HEAR 2021</h3>
                    {% include html_result_tables/esc50-v2.0.0-5h.html %}
                </div>
                <div id="esc50-v2.0.0-full" class="subtask">
                    <h3>Full Dataset</h3>
                    {% include html_result_tables/beehive_states_fold1-v2-5h.html %}
                </div>
            </div>

            <!-- FSD50k Full -->
            <div id="fsd50k-v1.0-full" class="leaderboard-task">
                <h2 class="task-title secret-task">FSD50k</h2>
                <div class="task-description">
                    <p><a href="https://zenodo.org/record/4060432#.YagAX_HMJpQ">FSD50K</a>
                        is a multilabel task. The dataset contains a large collection of
                        human-labeled sound events from the <a href="https://freesound.org/">
                            website</a>. Each sound event is labeled using classes from
                        the <a href="https://research.google.com/audioset/ontology/index.html">
                            AudioSet Ontology</a>, which consists of 200 classes.
                    </p>
                </div>
                {% include html_result_tables/fsd50k-v1.0-full.html %}
            </div>

            <!-- Gunshot Triangulation -->
            <div id="gunshot_triangulation-v1.0-full" class="leaderboard-task">
                <h2 class="task-title secret-task">Gunshot Triangulation - HEAR 2021</h2>
                <div class="task-description">
                    <p>The gunshot triangulation task is a novel multiclass classification
                    task that utilizes the dataset:
                        <a href="https://zenodo.org/record/3997406#.YagCO_HMJpR">
                            Gunshots recorded in an open field using iPod Touch devices
                        </a>. This dataset consists of 22 shots from 7 different firearms.
                        Each shot is recorded using four different iPod Touches located
                        at different distances from the shooter. The goal of this
                        task is to classify audio by the iPod Touch that recorded it.
                    </p>
                    <p>
                        The dataset was split into 7 different folds where each firearm
                        belonged to only one fold. Results are averaged over each fold.
                    </p>
                </div>
                {% include html_result_tables/gunshot_triangulation-v1.0-full.html %}
            </div>

            <!-- GTZAN -->
            <div id="tfds_gtzan-1.0.0-full" class="leaderboard-task">
                <h2 class="task-title secret-task">GTZAN Genre</h2>
                <div class="task-description">
                    <p>
                        This is a multiclass classification task. The
                        <a href="http://marsyas.info/downloads/datasets.html#gtzan-genre-collection">
                            GTZAN Genre Collection</a> is a dataset of 1000 audio tracks
                        (each 30 seconds in duration) that are categorized into
                        ten genres (100 tracks per genre).
                    </p>
                    <p>
                        We use the <a href="https://www.tensorflow.org/datasets/catalog/gtzan">
                        TensorFlow Dataset</a> version of this dataset. The dataset was split
                        into 10 folds and results were averaged over each of these folds.
                    </p>
                </div>
                {% include html_result_tables/tfds_gtzan-1.0.0-full.html %}
            </div>

            <!-- GTZAN Music - Speech -->
            <div id="tfds_gtzan_music_speech-1.0.0-full" class="leaderboard-task">
                <h2 class="task-title secret-task">GTZAN Music Speech </h2>
                <div class="task-description">
                    <p><a href="http://marsyas.info/downloads/datasets.html#music-speech">
                        GTZAN Music Speech</a> is a binary classification task. The goal
                    is to distinguish between music and speech. The dataset consists of
                    120 tracks (each 30 seconds in duration) and each class (music/speech)
                    has 60 examples.</p>
                    <p>
                        We use the <a href="https://www.tensorflow.org/datasets/catalog/gtzan_music_speech">
                        TensorFlow Dataset</a> version of this dataset. The dataset was split
                        into 10 folds and results were averaged over each of these folds.
                    </p>
                </div>
                {% include html_result_tables/tfds_gtzan_music_speech-1.0.0-full.html %}
            </div>

            <!-- Libricount -->
            <div id="libricount-v1.0.0-hear2021-full" class="leaderboard-task">
                <h2 class="task-title secret-task">LibriCount</h2>
                <div class="task-description"></div>
                {% include html_result_tables/libricount-v1.0.0-hear2021-full.html %}
            </div>

            <!-- Mridingam Stroke -->
            <div id="mridangam_stroke-v1.5-full" class="leaderboard-task">
                <h2 class="task-title secret-task">Mridingham Stroke</h2>
                <div class="task-description"></div>
                {% include html_result_tables/mridangam_stroke-v1.5-full.html %}
            </div>

            <!-- Mridingam Tonic -->
            <div id="mridangam_tonic-v1.5-full" class="leaderboard-task">
                <h2 class="task-title secret-task">Mridingham Tonic</h2>
                <div class="task-description"></div>
                {% include html_result_tables/mridangam_tonic-v1.5-full.html %}
            </div>

            <!-- Vocal Imitations -->
            <div id="vocal_imitation-v1.1.3-full" class="leaderboard-task">
                <h2 class="task-title secret-task">Vocal Imitations</h2>
                <div class="task-description"></div>
                {% include html_result_tables/vocal_imitation-v1.1.3-full.html %}
            </div>

            <!-- Vox Lingua Top10 -->
            <div id="vox_lingua_top10-hear2021-full" class="leaderboard-task">
                <h2 class="task-title secret-task">Vox Lingua Top 10</h2>
                <div class="task-description"></div>
                {% include html_result_tables/vox_lingua_top10-hear2021-full.html %}
            </div>
        </div>

    </div>
</article>


<!-- Regular Datatables -->
<link rel="stylesheet" href="https://cdn.datatables.net/1.11.1/css/jquery.dataTables.min.css">

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script src="https://cdn.datatables.net/1.11.1/js/jquery.dataTables.min.js"></script>

<!-- Bootstrap Datatables (Uncomment to try this style and comment out imports above)
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.2/css/bootstrap.css">
<link rel="stylesheet" href="https://cdn.datatables.net/1.11.1/css/dataTables.bootstrap4.min.css">

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script src="https://cdn.datatables.net/1.11.1/js/jquery.dataTables.min.js"></script>
<script src="https://cdn.datatables.net/1.11.1/js/dataTables.bootstrap4.min.js"></script>
-->

<!-- Custom CSS & JS -->
<link rel="stylesheet" href="{{ "/assets/hear2021.css" | prepend: site.baseurl | prepend: site.url }}">
<link rel="stylesheet" href="{{ "/assets/hear-leaderboard.css" | prepend: site.baseurl | prepend: site.url }}">
<script src="{{ "/assets/js/hear-results.js" | prepend: site.baseurl | prepend: site.url }}"></script>
